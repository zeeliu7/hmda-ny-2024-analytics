{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MrY_Jduikjnl"
   },
   "outputs": [],
   "source": [
    "# Data source: https://ffiec.cfpb.gov/data-browser/data/2024?category=states\n",
    "# Data fields: https://ffiec.cfpb.gov/documentation/publications/loan-level-datasets/lar-data-fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning and Handling Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gF_87bqMa9OR",
    "outputId": "2645d254-fc48-48ac-cc44-f7a43260df0a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('state_NY.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ueGs-M7ubB1Y",
    "outputId": "33981880-f615-4347-c2e1-70ae1fbc3173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383577, 99)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VSk2mu_eev1_"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['activity_year', 'lei', 'derived_msa-md', 'state_code', 'census_tract', 'total_loan_costs', \n",
    "              'total_points_and_fees','origination_charges', 'discount_points', 'lender_credits', \n",
    "              'prepayment_penalty_term', 'intro_rate_period', 'multifamily_affordable_units', \n",
    "              'applicant_credit_score_type', 'co-applicant_credit_score_type', 'submission_of_application',\n",
    "              'initially_payable_to_institution', 'hoepa_status', 'manufactured_home_secured_property_type', \n",
    "              'manufactured_home_land_property_interest'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383577, 79)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-EitHVcUl_Ql"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning derived_ethnicity\n",
      "(300539, 79)\n",
      "After cleaning derived_race\n",
      "(286487, 79)\n",
      "After cleaning derived_sex\n",
      "(285610, 79)\n",
      "After cleaning loan_purpose\n",
      "(285592, 79)\n",
      "After cleaning applicant_age\n",
      "(283286, 79)\n",
      "After cleaning co-applicant_age\n",
      "(283026, 79)\n"
     ]
    }
   ],
   "source": [
    "df = df[~df['derived_ethnicity'].isin(['Ethnicity Not Available'])]\n",
    "print(\"After cleaning derived_ethnicity\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df[~df['derived_race'].isin(['Race Not Available'])]\n",
    "print(\"After cleaning derived_race\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df[~df['derived_sex'].isin(['Sex Not Available'])]\n",
    "print(\"After cleaning derived_sex\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df[~df['loan_purpose'].astype(str).isin(['5'])]\n",
    "print(\"After cleaning loan_purpose\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df[~df['applicant_age'].astype(str).isin(['8888'])]\n",
    "print(\"After cleaning applicant_age\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df[~df['co-applicant_age'].astype(str).isin(['8888'])]\n",
    "print(\"After cleaning co-applicant_age\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CBRkXvO0te6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 79)\n"
     ]
    }
   ],
   "source": [
    "columns_require_1111_cleaning = ['reverse_mortgage', 'open-end_line_of_credit', 'business_or_commercial_purpose']\n",
    "columns_require_1111_cleaning.extend([f'aus-{i}' for i in range(1, 6)])\n",
    "columns_require_1111_cleaning.extend([f'denial_reason-{i}' for i in range(1, 5)])\n",
    "for col in columns_require_1111_cleaning:\n",
    "    df = df[~df[col].astype(str).isin(['1111'])]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tVIDRctT0UBh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 79)\n"
     ]
    }
   ],
   "source": [
    "columns_require_exempt_cleaning = ['loan_to_value_ratio', 'interest_rate', 'rate_spread', 'loan_term', \n",
    "                                   'property_value', 'debt_to_income_ratio']\n",
    "for col in columns_require_exempt_cleaning:\n",
    "    df = df[~df[col].astype(str).isin(['Exempt'])]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qkcrO2-OhYqD"
   },
   "outputs": [],
   "source": [
    "# source: https://en.wikipedia.org/wiki/List_of_counties_in_New_York\n",
    "\n",
    "df['county_code'] = df['county_code'].astype(str).map({\n",
    "    \"36001\": \"Albany\",\n",
    "    \"36003\": \"Allegany\",\n",
    "    \"36005\": \"Bronx\",\n",
    "    \"36007\": \"Broome\",\n",
    "    \"36009\": \"Cattaraugus\",\n",
    "    \"36011\": \"Cayuga\",\n",
    "    \"36013\": \"Chautauqua\",\n",
    "    \"36015\": \"Chemung\",\n",
    "    \"36017\": \"Chenango\",\n",
    "    \"36019\": \"Clinton\",\n",
    "    \"36021\": \"Columbia\",\n",
    "    \"36023\": \"Cortland\",\n",
    "    \"36025\": \"Delaware\",\n",
    "    \"36027\": \"Dutchess\",\n",
    "    \"36029\": \"Erie\",\n",
    "    \"36031\": \"Essex\",\n",
    "    \"36033\": \"Franklin\",\n",
    "    \"36035\": \"Fulton\",\n",
    "    \"36037\": \"Genesee\",\n",
    "    \"36039\": \"Greene\",\n",
    "    \"36041\": \"Hamilton\",\n",
    "    \"36043\": \"Herkimer\",\n",
    "    \"36045\": \"Jefferson\",\n",
    "    \"36047\": \"Kings\",\n",
    "    \"36049\": \"Lewis\",\n",
    "    \"36051\": \"Livingston\",\n",
    "    \"36053\": \"Madison\",\n",
    "    \"36055\": \"Monroe\",\n",
    "    \"36057\": \"Montgomery\",\n",
    "    \"36059\": \"Nassau\",\n",
    "    \"36061\": \"New_York\",\n",
    "    \"36063\": \"Niagara\",\n",
    "    \"36065\": \"Oneida\",\n",
    "    \"36067\": \"Onondaga\",\n",
    "    \"36069\": \"Ontario\",\n",
    "    \"36071\": \"Orange\",\n",
    "    \"36073\": \"Orleans\",\n",
    "    \"36075\": \"Oswego\",\n",
    "    \"36077\": \"Otsego\",\n",
    "    \"36079\": \"Putnam\",\n",
    "    \"36081\": \"Queens\",\n",
    "    \"36083\": \"Rensselaer\",\n",
    "    \"36085\": \"Richmond\",\n",
    "    \"36087\": \"Rockland\",\n",
    "    \"36089\": \"St_Lawrence\",\n",
    "    \"36091\": \"Saratoga\",\n",
    "    \"36093\": \"Schenectady\",\n",
    "    \"36095\": \"Schoharie\",\n",
    "    \"36097\": \"Schuyler\",\n",
    "    \"36099\": \"Seneca\",\n",
    "    \"36101\": \"Steuben\",\n",
    "    \"36103\": \"Suffolk\",\n",
    "    \"36105\": \"Sullivan\",\n",
    "    \"36107\": \"Tioga\",\n",
    "    \"36109\": \"Tompkins\",\n",
    "    \"36111\": \"Ulster\",\n",
    "    \"36113\": \"Warren\",\n",
    "    \"36115\": \"Washington\",\n",
    "    \"36117\": \"Wayne\",\n",
    "    \"36119\": \"Westchester\",\n",
    "    \"36121\": \"Wyoming\",\n",
    "    \"36123\": \"Yates\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9etQceJ3kBQj"
   },
   "outputs": [],
   "source": [
    "df['conforming_loan_limit'] = df['conforming_loan_limit'].map({\n",
    "    \"C\": \"Conforming\",\n",
    "    \"NC\": \"Nonconforming\",\n",
    "    \"U\": \"Undetermined\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1SgDqRPvn1ML"
   },
   "outputs": [],
   "source": [
    "df['action_taken'] = df['action_taken'].astype(str).map({\n",
    "    \"1\": \"Loan_originated\",\n",
    "    \"2\": \"Application_approved_but_not_accepted\",\n",
    "    \"3\": \"Application_denied\",\n",
    "    \"4\": \"Application_withdrawn_by_applicant\",\n",
    "    \"5\": \"File_closed_for_incompleteness\",\n",
    "    \"6\": \"Purchased_loan\",\n",
    "    \"7\": \"Preapproval_request_denied\",\n",
    "    \"8\": \"Preapproval_request_approved_but_not_accepted\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5-KdSB5Gox4m"
   },
   "outputs": [],
   "source": [
    "df['purchaser_type'] = df['purchaser_type'].astype(str).map({\n",
    "    \"0\": \"Not_applicable\",\n",
    "    \"1\": \"Fannie_Mae\",\n",
    "    \"2\": \"Ginnie_Mae\",\n",
    "    \"3\": \"Freddie_Mac\",\n",
    "    \"4\": \"Farmer_Mac\",\n",
    "    \"5\": \"Private_securitizer\",\n",
    "    \"6\": \"Commercial_bank_or_savings_bank_or_savings_association\",\n",
    "    \"71\": \"Credit_union_or_mortgage_company_or_finance_company\",\n",
    "    \"72\": \"Life_insurance_company\",\n",
    "    \"8\": \"Affiliate_institution\",\n",
    "    \"9\": \"Other_type_of_purchaser\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9_pW4jhorwtI"
   },
   "outputs": [],
   "source": [
    "df['loan_type'] = df['loan_type'].astype(str).map({\n",
    "    \"1\": \"Conventional\",\n",
    "    \"2\": \"FHA_insured\",\n",
    "    \"3\": \"VA_guaranteed\",\n",
    "    \"4\": \"RHS_or_FSA_guaranteed\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_6dS3uCZ4s9C"
   },
   "outputs": [],
   "source": [
    "df['occupancy_type'] = df['occupancy_type'].astype(str).map({\n",
    "    \"1\": \"Principal_residence\",\n",
    "    \"2\": \"Second_residence\",\n",
    "    \"3\": \"Investment_property\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "oS8t5iBBOJLj"
   },
   "outputs": [],
   "source": [
    "df['applicant_sex'] = df['applicant_sex'].astype(str).map({\n",
    "    \"1\": \"Male\",\n",
    "    \"2\": \"Female\",\n",
    "    \"3\": \"Not_provided\",\n",
    "    \"4\": \"Not_applicable\",\n",
    "    \"6\": \"Both_selected\"\n",
    "})\n",
    "\n",
    "df['co-applicant_sex'] = df['co-applicant_sex'].astype(str).map({\n",
    "    \"1\": \"Male\",\n",
    "    \"2\": \"Female\",\n",
    "    \"3\": \"Not_provided\",\n",
    "    \"4\": \"Not_applicable\",\n",
    "    \"5\": \"No_co-applicant\",\n",
    "    \"6\": \"Both_selected\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jAjSZnwxD46P"
   },
   "outputs": [],
   "source": [
    "df['applicant_ethnicity_observed'] = df['applicant_ethnicity_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan\n",
    "})\n",
    "\n",
    "df['co-applicant_ethnicity_observed'] = df['co-applicant_ethnicity_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan,\n",
    "    \"4\": \"no_co-applicant\"\n",
    "})\n",
    "\n",
    "df['applicant_race_observed'] = df['applicant_race_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan\n",
    "})\n",
    "\n",
    "df['co-applicant_race_observed'] = df['co-applicant_race_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan,\n",
    "    \"4\": \"no_co-applicant\"\n",
    "})\n",
    "\n",
    "df['applicant_sex_observed'] = df['applicant_sex_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan\n",
    "})\n",
    "\n",
    "df['co-applicant_sex_observed'] = df['co-applicant_sex_observed'].astype(str).map({\n",
    "    \"1\": True,\n",
    "    \"2\": False,\n",
    "    \"3\": np.nan,\n",
    "    \"4\": \"no_co-applicant\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UkncyHp9p9Rh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 79)\n"
     ]
    }
   ],
   "source": [
    "def convert_to_boolean_and_drop(df, conversions_list):\n",
    "    for new_col, original_col, value_for_true in conversions_list:\n",
    "        df[new_col] = (df[original_col].astype(str) == value_for_true)\n",
    "        df = df.drop(original_col, axis=1)\n",
    "    return df\n",
    "\n",
    "boolean_conversions = [\n",
    "    ('preapproval_requested', 'preapproval', '1'),\n",
    "    ('secured_by_a_first_lien', 'lien_status', '1'),\n",
    "    ('is_reverse_mortgage', 'reverse_mortgage', '1'),\n",
    "    ('is_open-end_line_of_credit', 'open-end_line_of_credit', '1'),\n",
    "    ('primarily_for_a_business_or_commercial_purpose', 'business_or_commercial_purpose', '1'),\n",
    "    ('includes_negative_amortization', 'negative_amortization', '1'),\n",
    "    ('includes_interest_only_payment', 'interest_only_payment', '1'),\n",
    "    ('includes_balloon_payment', 'balloon_payment', '1'),\n",
    "    ('includes_other_nonamortizing_features', 'other_nonamortizing_features', '1'),\n",
    "    ('is_site_built', 'construction_method', '1')\n",
    "]\n",
    "df = convert_to_boolean_and_drop(df, boolean_conversions)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3OcZM0PblZ2n"
   },
   "outputs": [],
   "source": [
    "columns_require_regex_cleaning = ['derived_loan_product_type', 'derived_dwelling_category', 'derived_ethnicity', \n",
    "                                  'derived_race', 'derived_sex']\n",
    "for col in columns_require_regex_cleaning:\n",
    "    df[col] = df[col].str.replace(r'[^a-zA-Z0-9\\-_]', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "bW7d_cGW766u"
   },
   "outputs": [],
   "source": [
    "df['total_units'] = df['total_units'].replace({\">149\": \"over_149\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "gIrA5LOxBXjx"
   },
   "outputs": [],
   "source": [
    "df['debt_to_income_ratio'] = df['debt_to_income_ratio'].replace({'>60%': 'over_60_percent', '<20%': \"below_20_percent\"})\n",
    "df['debt_to_income_ratio'] = df['debt_to_income_ratio'].replace(r'%', '_percent', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9Nn62OzfQFL7"
   },
   "outputs": [],
   "source": [
    "df['applicant_age'] = df['applicant_age'].replace({'<25': 'below_25', '>74': \"above_74\"})\n",
    "df['co-applicant_age'] = df['co-applicant_age'].replace({'<25': 'below_25', '>74': \"above_74\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "QKY7luPsHaLq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 87)\n"
     ]
    }
   ],
   "source": [
    "ethnicity_map = {\n",
    "    '1': 'Hispanic_or_Latino',\n",
    "    '11': 'Mexican',\n",
    "    '12': 'Puerto_Rican',\n",
    "    '13': 'Cuban',\n",
    "    '14': 'Other_Hispanic_or_Latino',\n",
    "    '2': 'Not_Hispanic_or_Latino',\n",
    "    '3': 'Not_provided',\n",
    "    '4': 'Not_applicable',\n",
    "    '5': \"No_co-applicant\"\n",
    "}\n",
    "\n",
    "applicant_ethnicity_cols = [f'applicant_ethnicity-{i}' for i in range(1, 6)]\n",
    "\n",
    "df[applicant_ethnicity_cols] = df[applicant_ethnicity_cols].astype(str)\n",
    "\n",
    "for code, category in ethnicity_map.items():\n",
    "    col_name = f'applicant_ethnicity_is_{category}'\n",
    "    df[col_name] = df[applicant_ethnicity_cols].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(applicant_ethnicity_cols, axis=1)\n",
    "\n",
    "co_applicant_ethnicity_cols = [f'co-applicant_ethnicity-{i}' for i in range(1, 6)]\n",
    "\n",
    "df[co_applicant_ethnicity_cols] = df[co_applicant_ethnicity_cols].astype(str)\n",
    "\n",
    "for code, category in ethnicity_map.items():\n",
    "    col_name = f'co-applicant_ethnicity_is_{category}'\n",
    "    df[col_name] = df[co_applicant_ethnicity_cols].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(co_applicant_ethnicity_cols, axis=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vFqYfTBrFiet"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 115)\n"
     ]
    }
   ],
   "source": [
    "race_map = {\n",
    "    '1': 'American_Indian_or_Alaska_Native',\n",
    "    '2': 'Asian',\n",
    "    '21': 'Asian_Indian',\n",
    "    '22': 'Chinese',\n",
    "    '23': 'Filipino',\n",
    "    '24': 'Japanese',\n",
    "    '25': 'Korean',\n",
    "    '26': 'Vietnamese',\n",
    "    '27': 'Other_Asian',\n",
    "    '3': 'Black_or_African_American',\n",
    "    '4': 'Native_Hawaiian_or_Other_Pacific_Islander',\n",
    "    '41': 'Native_Hawaiian',\n",
    "    '42': 'Guamanian_or_Chamorro',\n",
    "    '43': 'Samoan',\n",
    "    '44': 'Other_Pacific_Islander',\n",
    "    '5': 'White',\n",
    "    '6': 'Not_provided',\n",
    "    '7': 'Not_applicable',\n",
    "    '8': 'No_co-applicant'\n",
    "}\n",
    "\n",
    "applicant_race_cols = [f'applicant_race-{i}' for i in range(1, 6)]\n",
    "\n",
    "df[applicant_race_cols] = df[applicant_race_cols].astype(str)\n",
    "\n",
    "for code, category in race_map.items():\n",
    "    col_name = f'applicant_race_is_{category}'\n",
    "    df[col_name] = df[applicant_race_cols].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(applicant_race_cols, axis=1)\n",
    "\n",
    "co_applicant_race_cols = [f'co-applicant_race-{i}' for i in range(1, 6)]\n",
    "\n",
    "df[co_applicant_race_cols] = df[co_applicant_race_cols].astype(str)\n",
    "\n",
    "for code, category in race_map.items():\n",
    "    col_name = f'co-applicant_race_is_{category}'\n",
    "    df[col_name] = df[co_applicant_race_cols].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(co_applicant_race_cols, axis=1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "5vN3hUuxYtKB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/3549469057.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/3549469057.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/3549469057.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/3549469057.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/3549469057.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n"
     ]
    }
   ],
   "source": [
    "aus_map = {\n",
    "    '1': 'DU',\n",
    "    '2': 'LP_or_Loan_Product_Advisor',\n",
    "    '3': 'TOTAL_scorecard',\n",
    "    '4': 'GUS',\n",
    "    '5': 'Other',\n",
    "    '6': 'Not_applicable',\n",
    "    '7': 'Internal Proprietary System'\n",
    "}\n",
    "\n",
    "aus_rows = [f'aus-{i}' for i in range(1, 6)]\n",
    "\n",
    "df[aus_rows] = df[aus_rows].astype(str)\n",
    "\n",
    "for code, category in aus_map.items():\n",
    "    col_name = f'aus_is_{category}'\n",
    "    df[col_name] = df[aus_rows].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(aus_rows, axis=1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_11DL5KDbVEl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1366771317.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275232, 123)\n"
     ]
    }
   ],
   "source": [
    "denial_reason_map = {\n",
    "    '1': 'Debt-to-income_ratio',\n",
    "    '2': 'Employment_history',\n",
    "    '3': 'Credit_history',\n",
    "    '4': 'Collateral',\n",
    "    '5': 'Insufficient_cash',\n",
    "    '6': 'Unverifiable_information',\n",
    "    '7': 'Credit_application_incomplete',\n",
    "    '8': 'Mortgage_insurance_denied',\n",
    "    '9': 'Other',\n",
    "    '10': 'Not_applicable'\n",
    "}\n",
    "\n",
    "denial_reason_rows = [f'denial_reason-{i}' for i in range(1, 5)]\n",
    "\n",
    "df[denial_reason_rows] = df[denial_reason_rows].astype(str)\n",
    "\n",
    "for code, category in denial_reason_map.items():\n",
    "    col_name = f'denial_reason_is_{category}'\n",
    "    df[col_name] = df[denial_reason_rows].isin([code]).any(axis=1)\n",
    "\n",
    "df = df.drop(denial_reason_rows, axis=1)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   county_code conforming_loan_limit derived_loan_product_type  \\\n",
      "0     New_York            Conforming   Conventional_First_Lien   \n",
      "1   Rensselaer            Conforming   Conventional_First_Lien   \n",
      "2     New_York            Conforming   Conventional_First_Lien   \n",
      "3      Suffolk            Conforming   Conventional_First_Lien   \n",
      "4  Westchester            Conforming   Conventional_First_Lien   \n",
      "\n",
      "              derived_dwelling_category       derived_ethnicity  \\\n",
      "0  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "1  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "2  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "3  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "4  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "\n",
      "                derived_race derived_sex                        action_taken  \\\n",
      "0                      White       Joint                     Loan_originated   \n",
      "1  Black_or_African_American        Male                     Loan_originated   \n",
      "2                      White       Joint  Application_withdrawn_by_applicant   \n",
      "3                      White       Joint  Application_withdrawn_by_applicant   \n",
      "4  Black_or_African_American      Female  Application_withdrawn_by_applicant   \n",
      "\n",
      "                                      purchaser_type     loan_type  ...  \\\n",
      "0  Commercial_bank_or_savings_bank_or_savings_ass...  Conventional  ...   \n",
      "1  Credit_union_or_mortgage_company_or_finance_co...  Conventional  ...   \n",
      "2                                     Not_applicable  Conventional  ...   \n",
      "3                                     Not_applicable  Conventional  ...   \n",
      "4                                     Not_applicable  Conventional  ...   \n",
      "\n",
      "  denial_reason_is_Debt-to-income_ratio denial_reason_is_Employment_history  \\\n",
      "0                                 False                               False   \n",
      "1                                 False                               False   \n",
      "2                                 False                               False   \n",
      "3                                 False                               False   \n",
      "4                                 False                               False   \n",
      "\n",
      "  denial_reason_is_Credit_history denial_reason_is_Collateral  \\\n",
      "0                           False                       False   \n",
      "1                           False                       False   \n",
      "2                           False                       False   \n",
      "3                           False                       False   \n",
      "4                           False                       False   \n",
      "\n",
      "  denial_reason_is_Insufficient_cash  \\\n",
      "0                              False   \n",
      "1                              False   \n",
      "2                              False   \n",
      "3                              False   \n",
      "4                              False   \n",
      "\n",
      "  denial_reason_is_Unverifiable_information  \\\n",
      "0                                     False   \n",
      "1                                     False   \n",
      "2                                     False   \n",
      "3                                     False   \n",
      "4                                     False   \n",
      "\n",
      "  denial_reason_is_Credit_application_incomplete  \\\n",
      "0                                          False   \n",
      "1                                          False   \n",
      "2                                          False   \n",
      "3                                          False   \n",
      "4                                          False   \n",
      "\n",
      "  denial_reason_is_Mortgage_insurance_denied denial_reason_is_Other  \\\n",
      "0                                      False                  False   \n",
      "1                                      False                  False   \n",
      "2                                      False                  False   \n",
      "3                                      False                  False   \n",
      "4                                      False                  False   \n",
      "\n",
      "  denial_reason_is_Not_applicable  \n",
      "0                            True  \n",
      "1                            True  \n",
      "2                            True  \n",
      "3                            True  \n",
      "4                            True  \n",
      "\n",
      "[5 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['county_code', 'conforming_loan_limit', 'income', 'debt_to_income_ratio', 'applicant_age', \n",
    "                       'co-applicant_age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "28jFiXQFcKiO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216635, 123)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qS6jCwfUcLBP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   county_code conforming_loan_limit      derived_loan_product_type  \\\n",
      "0     New_York            Conforming        Conventional_First_Lien   \n",
      "1   Rensselaer            Conforming        Conventional_First_Lien   \n",
      "15        Erie            Conforming  Conventional_Subordinate_Lien   \n",
      "16        Erie            Conforming        Conventional_First_Lien   \n",
      "18  Chautauqua            Conforming  Conventional_Subordinate_Lien   \n",
      "\n",
      "               derived_dwelling_category       derived_ethnicity  \\\n",
      "0   Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "1   Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "15  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "16  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "18  Single_Family__1-4_Units__Site-Built  Not_Hispanic_or_Latino   \n",
      "\n",
      "                 derived_race derived_sex     action_taken  \\\n",
      "0                       White       Joint  Loan_originated   \n",
      "1   Black_or_African_American        Male  Loan_originated   \n",
      "15                      White       Joint  Loan_originated   \n",
      "16                      White        Male  Loan_originated   \n",
      "18                      White       Joint  Loan_originated   \n",
      "\n",
      "                                       purchaser_type     loan_type  ...  \\\n",
      "0   Commercial_bank_or_savings_bank_or_savings_ass...  Conventional  ...   \n",
      "1   Credit_union_or_mortgage_company_or_finance_co...  Conventional  ...   \n",
      "15                                     Not_applicable  Conventional  ...   \n",
      "16                                     Not_applicable  Conventional  ...   \n",
      "18                                     Not_applicable  Conventional  ...   \n",
      "\n",
      "   denial_reason_is_Debt-to-income_ratio denial_reason_is_Employment_history  \\\n",
      "0                                  False                               False   \n",
      "1                                  False                               False   \n",
      "15                                 False                               False   \n",
      "16                                 False                               False   \n",
      "18                                 False                               False   \n",
      "\n",
      "   denial_reason_is_Credit_history denial_reason_is_Collateral  \\\n",
      "0                            False                       False   \n",
      "1                            False                       False   \n",
      "15                           False                       False   \n",
      "16                           False                       False   \n",
      "18                           False                       False   \n",
      "\n",
      "   denial_reason_is_Insufficient_cash  \\\n",
      "0                               False   \n",
      "1                               False   \n",
      "15                              False   \n",
      "16                              False   \n",
      "18                              False   \n",
      "\n",
      "   denial_reason_is_Unverifiable_information  \\\n",
      "0                                      False   \n",
      "1                                      False   \n",
      "15                                     False   \n",
      "16                                     False   \n",
      "18                                     False   \n",
      "\n",
      "   denial_reason_is_Credit_application_incomplete  \\\n",
      "0                                           False   \n",
      "1                                           False   \n",
      "15                                          False   \n",
      "16                                          False   \n",
      "18                                          False   \n",
      "\n",
      "   denial_reason_is_Mortgage_insurance_denied denial_reason_is_Other  \\\n",
      "0                                       False                  False   \n",
      "1                                       False                  False   \n",
      "15                                      False                  False   \n",
      "16                                      False                  False   \n",
      "18                                      False                  False   \n",
      "\n",
      "   denial_reason_is_Not_applicable  \n",
      "0                             True  \n",
      "1                             True  \n",
      "15                            True  \n",
      "16                            True  \n",
      "18                            True  \n",
      "\n",
      "[5 rows x 123 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('hmda_ny_2024_cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "cols = ['loan_to_value_ratio','interest_rate','rate_spread','property_value']\n",
    "\n",
    "missing_mask = df[cols].isna().any(axis=1)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "df.loc[missing_mask, cols] = imputer.fit_transform(df.loc[missing_mask, cols]).round(2)\n",
    "\n",
    "mode_value = df['loan_term'].mode()[0]\n",
    "df['loan_term'] = df['loan_term'].fillna(mode_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1922800209.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['approved'] = (df['action_taken'] == 'Loan_originated').astype(int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "approved\n",
       "1    156349\n",
       "0     53020\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df[df['action_taken'].isin([\n",
    "    'Loan_originated',\n",
    "    'Application_denied'\n",
    "])]\n",
    "\n",
    "df['approved'] = (df['action_taken'] == 'Loan_originated').astype(int)\n",
    "df['approved'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/141426733.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['income_log'] = np.log1p(df['income'])\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/141426733.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['property_value_log'] = np.log1p(df['property_value'])\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df['income'] = pd.to_numeric(df['income'], errors='coerce')\n",
    "df['property_value'] = pd.to_numeric(df['property_value'], errors='coerce')\n",
    "\n",
    "df['income_log'] = np.log1p(df['income'])\n",
    "df['property_value_log'] = np.log1p(df['property_value'])\n",
    "df['income_log'] = np.log1p(df['income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(\n",
    "    df,\n",
    "    columns=['loan_type'],\n",
    "    drop_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dummies = pd.get_dummies(\n",
    "    df['co-applicant_age'],\n",
    "    prefix='co_age',\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "df = pd.concat([df, age_dummies], axis=1)\n",
    "\n",
    "age_dummies = pd.get_dummies(\n",
    "    df['applicant_age'],\n",
    "    prefix='app_age',\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "df = pd.concat([df, age_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['co-applicant_race_observed'] = (\n",
    "    df['co-applicant_race_observed']\n",
    "    .replace('nan', np.nan)\n",
    ")\n",
    "df['co_applicant'] = (\n",
    "    df['co-applicant_race_observed']\n",
    "    .ne('no_co-applicant')\n",
    "    .astype(int)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_mask = df['debt_to_income_ratio'].str.match(r'^\\d+(\\.\\d+)?$')\n",
    "\n",
    "df_numeric = df[numeric_mask].copy()\n",
    "\n",
    "df_numeric['dti_numeric'] = df_numeric['debt_to_income_ratio'].astype(float)\n",
    "\n",
    "df_numeric['dti_group'] = pd.cut(\n",
    "    df_numeric['dti_numeric'],\n",
    "    bins=[0, 20, 30, 36, 43, 50, 100],\n",
    "    labels=['<20', '20-30', '30-36', '36-43', '43-50', '50+']\n",
    ")\n",
    "dti_dummies = pd.get_dummies(\n",
    "    df_numeric['dti_group'],\n",
    "    prefix='dti',\n",
    "    drop_first=True\n",
    ")\n",
    "\n",
    "df_numeric = pd.concat([df_numeric, dti_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='bool').columns:\n",
    "    df[col] = df[col].astype(int)\n",
    "bool_cols = df.select_dtypes(include='bool').columns\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\n",
    "    'co_age_9999',\n",
    "    'applicant_age_above_62',\n",
    "    'co-applicant_age_above_62'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1576867471.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['loan_to_income'] = df['loan_amount'] / (df['income'] + 1e-9)\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1576867471.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['equity'] = df['property_value'] - df['loan_amount']\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/1576867471.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['equity_ratio'] = 1 - (df['loan_amount'] / (df['property_value'] + 1e-9))\n"
     ]
    }
   ],
   "source": [
    "for c in ['loan_amount', 'interest_rate', 'loan_term', 'income', 'property_value', 'loan_to_value_ratio']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "if 'loan_amount' in df.columns and 'income' in df.columns:\n",
    "    df['loan_to_income'] = df['loan_amount'] / (df['income'] + 1e-9)\n",
    "\n",
    "if 'property_value' in df.columns and 'loan_amount' in df.columns:\n",
    "    df['equity'] = df['property_value'] - df['loan_amount']\n",
    "    df['equity_ratio'] = 1 - (df['loan_amount'] / (df['property_value'] + 1e-9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2787598137.py:6: RuntimeWarning: invalid value encountered in divide\n",
      "  P * r * (1 + r)**n / ((1 + r)**n - 1),\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2787598137.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['monthly_payment_est'] = monthly_payment(\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2787598137.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['pti'] = 12 * df['monthly_payment_est'] / (df['income'] + 1e-9)\n"
     ]
    }
   ],
   "source": [
    "def monthly_payment(P, annual_rate_pct, n_months):\n",
    "    r = (annual_rate_pct / 100.0) / 12.0\n",
    "    n = n_months\n",
    "    return np.where(\n",
    "        (P > 0) & (n > 0) & (r > 0),\n",
    "        P * r * (1 + r)**n / ((1 + r)**n - 1),\n",
    "        np.where((P > 0) & (n > 0) & (r == 0), P / n, np.nan)\n",
    "    )\n",
    "needed = {'loan_amount', 'interest_rate', 'loan_term', 'income'}\n",
    "if needed.issubset(df.columns):\n",
    "    df['monthly_payment_est'] = monthly_payment(\n",
    "        df['loan_amount'].values,\n",
    "        df['interest_rate'].values,\n",
    "        df['loan_term'].values\n",
    "    )\n",
    "    df['pti'] = 12 * df['monthly_payment_est'] / (df['income'] + 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2664568481.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c + '_minus_county_median'] = df[c] - grp_median\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2664568481.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c + '_minus_county_median'] = df[c] - grp_median\n",
      "/var/folders/7x/4f_ykt_s0_xf7fhvb_zwgqrr0000gn/T/ipykernel_1716/2664568481.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[c + '_minus_county_median'] = df[c] - grp_median\n"
     ]
    }
   ],
   "source": [
    "group_col = 'county_code'\n",
    "for c in ['interest_rate','property_value','income']:\n",
    "    if group_col in df.columns and c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        grp_median = df.groupby(group_col)[c].transform('median')\n",
    "        df[c + '_minus_county_median'] = df[c] - grp_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added standardized columns: 16\n",
      "Excluded (not standardized) count: 100\n",
      "Example added cols: ['loan_amount_z', 'loan_to_value_ratio_z', 'interest_rate_z', 'loan_term_z', 'property_value_z', 'income_z', 'income_log_z', 'property_value_log_z', 'loan_to_income_z', 'equity_z']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_scaled = df.copy()\n",
    "\n",
    "num_cols = df_scaled.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "exclude = []\n",
    "for c in num_cols:\n",
    "    unique_vals = df_scaled[c].dropna().unique()\n",
    "    if len(unique_vals) <= 2 and set(unique_vals).issubset({0, 1}):\n",
    "        exclude.append(c)\n",
    "    if c.endswith('_missing') or c.endswith('_flag') or c.startswith('has_'):\n",
    "        exclude.append(c)\n",
    "\n",
    "exclude = list(set(exclude))\n",
    "scale_cols = [c for c in num_cols if c not in exclude]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = df_scaled[scale_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_filled = X.fillna(X.median(numeric_only=True))\n",
    "Z = scaler.fit_transform(X_filled)\n",
    "z_cols = [c + \"_z\" for c in scale_cols]  \n",
    "df_scaled[z_cols] = pd.DataFrame(Z, columns=z_cols, index=df_scaled.index)\n",
    "df = df_scaled.copy()\n",
    "\n",
    "print(\"Added standardized columns:\", len(z_cols))\n",
    "print(\"Excluded (not standardized) count:\", len(exclude))\n",
    "print(\"Example added cols:\", z_cols[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Exploratory Data Analysis (EDA)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
